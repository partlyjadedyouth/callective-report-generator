#!/usr/bin/env python3
"""
Interview Analysis Script - Analyzes participant app usage data from week 0-12

This script analyzes app usage data for all participants across weeks 0-12 and generates
individual participant reports with emotion records, app usage rankings, time-based analysis,
emotion percentages, and intervention activity percentages.
"""

import pandas as pd
import os
import sys
from datetime import datetime
from collections import Counter

# Add parent directory to path for imports
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class ParticipantAnalyzer:
    def __init__(self, base_path="/Users/joon/Developer/callective/report_generator"):
        """
        Initialize the analyzer with base paths for data files

        Args:
            base_path (str): Base path to the project directory
        """
        self.base_path = base_path
        self.csv_path = os.path.join(base_path, "data", "csv")
        self.output_path = os.path.join(base_path, "data", "interview")

        # Define emotion categories for analysis
        self.positive_emotions = ["í–‰ë³µí•´ìš”", "ì‹ ë‚˜ìš”", "ë§Œì¡±ìŠ¤ëŸ¬ì›Œìš”", "ì°¨ë¶„í•´ìš”"]
        self.negative_emotions = ["ìŠ¬í¼ìš”", "ìš°ìš¸í•´ìš”", "ë¶ˆì•ˆí•´ìš”", "í™”ê°€ë‚˜ìš”"]

        # Initialize data containers
        self.participants_data = {}
        self.all_app_usage_data = []
        self.emotion_records = []

    def load_participants(self):
        """Load participant information from CSV file"""
        participants_file = os.path.join(self.csv_path, "participants.csv")
        try:
            # Read participants data with proper encoding
            participants_df = pd.read_csv(participants_file, encoding="utf-8")

            # Create participant mapping dictionary
            for _, row in participants_df.iterrows():
                participant_id = row["ì‹ë³„ ê¸°í˜¸"]  # P1, P2, etc.
                participant_name = row["ì„±í•¨"]  # Name in Korean
                participant_login_id = row["ì•„ì´ë””"]  # Login ID

                self.participants_data[participant_id] = {
                    "name": participant_name,
                    "login_id": participant_login_id,
                    "phone": row["íœ´ëŒ€ì „í™” ë„¤ìë¦¬"],
                    "team": row["ì†Œì†"],
                    "role": row["ì§ë¬´"],
                    "gender": row["ì„±ë³„"],
                }

            print(f"âœ“ Loaded {len(self.participants_data)} participants")
            return True

        except Exception as e:
            print(f"âœ— Error loading participants: {e}")
            return False

    def load_app_usage_data(self):
        """Load app usage data from all weekly CSV files"""
        # Define the weeks we have data for
        weeks = ["0ì£¼ì°¨", "2ì£¼ì°¨", "4ì£¼ì°¨", "6ì£¼ì°¨", "8ì£¼ì°¨", "10ì£¼ì°¨", "12ì£¼ì°¨"]

        for week in weeks:
            filename = f"app_usage_data_{week}.csv"
            filepath = os.path.join(self.csv_path, filename)

            if os.path.exists(filepath):
                try:
                    # Read the CSV file
                    df = pd.read_csv(filepath, encoding="utf-8")

                    # Normalize column names - weeks 6-12 have underscores instead of spaces
                    column_mapping = {
                        'ì„ íƒí•œ_ê°ì •': 'ì„ íƒí•œ ê°ì •',
                        'ë‚ ì§œ_ì‹œê°„': 'ë‚ ì§œ / ì‹œê°„', 
                        'ì¤‘ì¬_í™œë™_ì´ë¦„': 'ì¤‘ì¬ í™œë™ ì´ë¦„',
                        'ë§ˆìŒ_ê¸°ë¡_ì…ë ¥_ë‚´ìš©_(í…ìŠ¤íŠ¸)': 'ë§ˆìŒ ê¸°ë¡ ì…ë ¥ ë‚´ìš© (í…ìŠ¤íŠ¸)'
                    }
                    
                    # Rename columns to standardize format
                    df = df.rename(columns=column_mapping)

                    # Add week information to each record
                    df["ì£¼ì°¨"] = week

                    # Append to our collection
                    self.all_app_usage_data.append(df)

                    print(f"âœ“ Loaded {filename}: {len(df)} records")

                except Exception as e:
                    print(f"âœ— Error loading {filename}: {e}")
            else:
                print(f"âš  File not found: {filename}")

        # Combine all app usage data
        if self.all_app_usage_data:
            self.combined_app_data = pd.concat(
                self.all_app_usage_data, ignore_index=True
            )
            print(f"âœ“ Total app usage records: {len(self.combined_app_data)}")
        else:
            print("âœ— No app usage data loaded")

    def analyze_participant_emotions(self, participant_id, participant_name, login_id):
        """
        Analyze emotion records for a specific participant

        Args:
            participant_id (str): Participant ID (P1, P2, etc.)
            participant_name (str): Participant name
            login_id (str): Participant login ID

        Returns:
            dict: Analysis results for the participant
        """
        # Filter data for this participant using both name and ID matching (fix boolean logic)
        participant_records = self.combined_app_data[
            (self.combined_app_data["ì„±í•¨"] == participant_name) &
            (self.combined_app_data["ID"] == login_id)
        ].copy()

        # Basic emotion counts
        total_records = len(participant_records)

        # Count positive and negative emotions from emotion records only
        emotion_series = participant_records["ì„ íƒí•œ ê°ì •"].dropna()
        positive_count = sum(1 for emotion in emotion_series if emotion in self.positive_emotions)
        negative_count = sum(1 for emotion in emotion_series if emotion in self.negative_emotions)

        # Emotion percentages
        emotion_counts = Counter(participant_records["ì„ íƒí•œ ê°ì •"].dropna())
        emotion_percentages = {}

        if total_records > 0:
            for emotion, count in emotion_counts.items():
                emotion_percentages[emotion] = (count / total_records) * 100

        # ë§ˆìŒ ê¸°ë¡ (heart records) - written emotional content
        heart_records = []
        for _, record in participant_records.iterrows():
            if (
                pd.notna(record["ë§ˆìŒ ê¸°ë¡ ì…ë ¥ ë‚´ìš© (í…ìŠ¤íŠ¸)"])
                and record["ë§ˆìŒ ê¸°ë¡ ì…ë ¥ ë‚´ìš© (í…ìŠ¤íŠ¸)"].strip()
            ):
                heart_record = {
                    "content": record["ë§ˆìŒ ê¸°ë¡ ì…ë ¥ ë‚´ìš© (í…ìŠ¤íŠ¸)"],
                    "timestamp": record["ë‚ ì§œ / ì‹œê°„"],
                    "emotion": record["ì„ íƒí•œ ê°ì •"],
                }
                heart_records.append(heart_record)

        # ì¤‘ì¬ í™œë™ (intervention activities) analysis
        intervention_activities = participant_records["ì¤‘ì¬ í™œë™ ì´ë¦„"].dropna()
        intervention_counts = Counter(intervention_activities)
        intervention_percentages = {}

        total_interventions = len(intervention_activities)
        if total_interventions > 0:
            for activity, count in intervention_counts.items():
                intervention_percentages[activity] = (count / total_interventions) * 100

        # Time-based analysis (by hour ranges)
        time_analysis = self.analyze_time_patterns(participant_records)

        return {
            "total_records": total_records,
            "positive_records": positive_count,
            "negative_records": negative_count,
            "emotion_percentages": emotion_percentages,
            "heart_records": heart_records,
            "intervention_percentages": intervention_percentages,
            "time_analysis": time_analysis,
            "raw_data_count": len(participant_records),
        }

    def analyze_time_patterns(self, participant_records):
        """
        Analyze emotion patterns by time ranges

        Args:
            participant_records (DataFrame): Records for a specific participant

        Returns:
            dict: Time-based emotion analysis
        """
        # Use work-hour based time ranges matching existing analysis
        time_ranges = {
            "08:00-10:30 (ì˜¤ì „)": [],
            "10:30-12:00 (ì˜¤ì „)": [],
            "12:00-13:30 (ì ì‹¬)": [],
            "13:30-15:00 (ì˜¤í›„)": [],
            "15:00-16:30 (ì˜¤í›„)": [],
            "16:30-19:00 (ì €ë…)": [],
        }

        for _, record in participant_records.iterrows():
            if pd.notna(record["ë‚ ì§œ / ì‹œê°„"]):
                try:
                    # Parse the datetime string
                    dt_str = str(record["ë‚ ì§œ / ì‹œê°„"])
                    # Handle various datetime formats
                    if " " in dt_str:
                        time_part = dt_str.split(" ")[1]
                        hour = int(time_part.split(":")[0])

                        emotion = record["ì„ íƒí•œ ê°ì •"]

                        # Parse time more precisely for work-hour analysis
                        time_parts = time_part.split(":")
                        hour = int(time_parts[0])
                        minute = int(time_parts[1]) if len(time_parts) > 1 else 0
                        time_minutes = hour * 60 + minute
                        
                        # Categorize into work-hour time ranges
                        if 8 * 60 <= time_minutes < 10 * 60 + 30:  # 08:00-10:30
                            time_ranges["08:00-10:30 (ì˜¤ì „)"].append(emotion)
                        elif 10 * 60 + 30 <= time_minutes < 12 * 60:  # 10:30-12:00
                            time_ranges["10:30-12:00 (ì˜¤ì „)"].append(emotion)
                        elif 12 * 60 <= time_minutes < 13 * 60 + 30:  # 12:00-13:30
                            time_ranges["12:00-13:30 (ì ì‹¬)"].append(emotion)
                        elif 13 * 60 + 30 <= time_minutes < 15 * 60:  # 13:30-15:00
                            time_ranges["13:30-15:00 (ì˜¤í›„)"].append(emotion)
                        elif 15 * 60 <= time_minutes < 16 * 60 + 30:  # 15:00-16:30
                            time_ranges["15:00-16:30 (ì˜¤í›„)"].append(emotion)
                        elif 16 * 60 + 30 <= time_minutes < 19 * 60:  # 16:30-19:00
                            time_ranges["16:30-19:00 (ì €ë…)"].append(emotion)

                except (ValueError, IndexError):
                    continue  # Skip invalid datetime formats

        # Count emotions for each time range
        time_analysis = {}
        for time_range, emotions in time_ranges.items():
            time_analysis[time_range] = {
                "total": len(emotions),
                "emotions": Counter(emotions),
            }

        return time_analysis

    def calculate_app_usage_ranking(self):
        """Calculate app usage ranking for all participants"""
        # Count total records per participant
        participant_usage = {}

        for participant_id, participant_info in self.participants_data.items():
            participant_name = participant_info["name"]
            login_id = participant_info["login_id"]

            # Count records for this participant (fix boolean logic)
            participant_records = self.combined_app_data[
                (self.combined_app_data["ì„±í•¨"] == participant_name) &
                (self.combined_app_data["ID"] == login_id)
            ]

            participant_usage[participant_id] = len(participant_records)

        # Sort participants by usage (descending)
        sorted_usage = sorted(
            participant_usage.items(), key=lambda x: x[1], reverse=True
        )

        # Create ranking dictionary
        rankings = {}
        for rank, (participant_id, usage_count) in enumerate(sorted_usage, 1):
            rankings[participant_id] = {
                "rank": rank,
                "usage_count": usage_count,
                "total_participants": len(self.participants_data),
            }

        return rankings

    def generate_participant_report(self, participant_id):
        """
        Generate a comprehensive report for a specific participant

        Args:
            participant_id (str): Participant ID (P1, P2, etc.)
        """
        if participant_id not in self.participants_data:
            print(f"âœ— Participant {participant_id} not found")
            return

        participant_info = self.participants_data[participant_id]
        participant_name = participant_info["name"]
        login_id = participant_info["login_id"]

        print(f"ğŸ“Š Analyzing {participant_id} {participant_name}...")

        # Perform analysis
        analysis = self.analyze_participant_emotions(
            participant_id, participant_name, login_id
        )
        rankings = self.calculate_app_usage_ranking()

        # Get participant ranking
        participant_ranking = rankings.get(
            participant_id, {"rank": "N/A", "usage_count": 0}
        )

        # Generate report content
        report_lines = []
        report_lines.append(
            f"=== {participant_id} {participant_name} ì•± ì‚¬ìš© ë¶„ì„ ë³´ê³ ì„œ ==="
        )
        report_lines.append(f"ë¶„ì„ ê¸°ê°„: 0ì£¼ì°¨ - 12ì£¼ì°¨")
        report_lines.append(
            f"ìƒì„± ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        )
        report_lines.append("")

        # 1. ê°ì • ê¸°ë¡ í†µê³„
        report_lines.append("ğŸ“ˆ ê°ì • ê¸°ë¡ í†µê³„ (0ì£¼ì°¨-12ì£¼ì°¨)")
        report_lines.append(f"â€¢ ì´ ê°ì • ê¸°ë¡ ìˆ˜: {analysis['total_records']}ê°œ")
        report_lines.append(f"â€¢ ê¸ì •ì  ê°ì • ê¸°ë¡: {analysis['positive_records']}ê°œ")
        report_lines.append(f"â€¢ ë¶€ì •ì  ê°ì • ê¸°ë¡: {analysis['negative_records']}ê°œ")
        report_lines.append("")

        # 2. ì•± ì‚¬ìš© ìˆœìœ„
        report_lines.append("ğŸ† ì•± ì‚¬ìš© ìˆœìœ„")
        report_lines.append(
            f"â€¢ ì „ì²´ ì°¸ê°€ì ì¤‘ {participant_ranking['rank']}ìœ„ (ì´ {participant_ranking.get('total_participants', 43)}ëª… ì¤‘)"
        )
        report_lines.append(
            f"â€¢ ì´ ì•± ì‚¬ìš© ê¸°ë¡: {participant_ranking['usage_count']}íšŒ"
        )
        report_lines.append("")

        # 3. ì‹œê°„ëŒ€ë³„ ê°ì • ê¸°ë¡ ë¶„ì„
        report_lines.append("â° ì‹œê°„ëŒ€ë³„ ê°ì • ê¸°ë¡ ë¶„ì„")
        for time_range, data in analysis["time_analysis"].items():
            if data["total"] > 0:
                report_lines.append(f"â€¢ {time_range}: {data['total']}ê°œ ê¸°ë¡")
                for emotion, count in data["emotions"].most_common(3):  # Top 3 emotions
                    percentage = (count / data["total"]) * 100
                    report_lines.append(f"  - {emotion}: {count}íšŒ ({percentage:.1f}%)")
            else:
                report_lines.append(f"â€¢ {time_range}: ê¸°ë¡ ì—†ìŒ")
        report_lines.append("")

        # 4. ë§ˆìŒ ê¸°ë¡ ë‚´ìš©
        report_lines.append("ğŸ’­ ë§ˆìŒ ê¸°ë¡ ë‚´ìš©")
        if analysis["heart_records"]:
            report_lines.append(f"ì´ {len(analysis['heart_records'])}ê°œì˜ ë§ˆìŒ ê¸°ë¡:")
            for i, record in enumerate(
                analysis["heart_records"][:10], 1
            ):  # Show up to 10 records
                report_lines.append(f"{i}. [{record['timestamp']}] {record['emotion']}")
                report_lines.append(f"   \"{record['content']}\"")
                if i < len(analysis["heart_records"]):
                    report_lines.append("")

            if len(analysis["heart_records"]) > 10:
                report_lines.append(
                    f"... ë° {len(analysis['heart_records']) - 10}ê°œ ì¶”ê°€ ê¸°ë¡"
                )
        else:
            report_lines.append("ë§ˆìŒ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        report_lines.append("")

        # 5. ê°ì •ë³„ ë¹„ìœ¨
        report_lines.append("ğŸ˜Š ê°ì •ë³„ ê¸°ë¡ ë¹„ìœ¨")
        if analysis["emotion_percentages"]:
            # Sort emotions by percentage (descending)
            sorted_emotions = sorted(
                analysis["emotion_percentages"].items(),
                key=lambda x: x[1],
                reverse=True,
            )
            for emotion, percentage in sorted_emotions:
                report_lines.append(f"â€¢ {emotion}: {percentage:.1f}%")
        else:
            report_lines.append("ê°ì • ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")
        report_lines.append("")

        # 6. ì¤‘ì¬ í™œë™ ë¹„ìœ¨
        report_lines.append("ğŸ¯ ì¤‘ì¬ í™œë™ ì°¸ì—¬ ë¹„ìœ¨")
        if analysis["intervention_percentages"]:
            # Sort interventions by percentage (descending)
            sorted_interventions = sorted(
                analysis["intervention_percentages"].items(),
                key=lambda x: x[1],
                reverse=True,
            )
            for activity, percentage in sorted_interventions:
                report_lines.append(f"â€¢ {activity}: {percentage:.1f}%")
        else:
            report_lines.append("ì¤‘ì¬ í™œë™ ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤.")

        # Write report to file
        participant_dir = os.path.join(
            self.output_path, f"{participant_id} {participant_name}"
        )
        if not os.path.exists(participant_dir):
            os.makedirs(participant_dir)

        report_filename = f"{participant_id}_{participant_name}_ë¶„ì„ë³´ê³ ì„œ.txt"
        report_filepath = os.path.join(participant_dir, report_filename)

        try:
            with open(report_filepath, "w", encoding="utf-8") as f:
                f.write("\n".join(report_lines))

            print(f"âœ“ Report generated: {report_filepath}")

        except Exception as e:
            print(f"âœ— Error writing report for {participant_id}: {e}")

    def run_analysis(self):
        """Run the complete analysis for all participants"""
        print("ğŸš€ Starting Interview Analysis...")
        print("=" * 50)

        # Load participants data
        if not self.load_participants():
            return False

        # Load app usage data
        self.load_app_usage_data()

        if not hasattr(self, "combined_app_data") or self.combined_app_data.empty:
            print("âœ— No app usage data available for analysis")
            return False

        print("\nğŸ“Š Generating participant reports...")
        print("=" * 50)

        # Generate reports for all participants
        success_count = 0
        for participant_id in self.participants_data.keys():
            try:
                self.generate_participant_report(participant_id)
                success_count += 1
            except Exception as e:
                print(f"âœ— Error analyzing {participant_id}: {e}")

        print(f"\nâœ… Analysis complete!")
        print(
            f"Successfully generated {success_count}/{len(self.participants_data)} participant reports"
        )
        print(f"Reports saved to: {self.output_path}")

        return True


def main():
    """Main function to run the analysis"""
    print("Interview Participant Analysis Script")
    print("=" * 40)

    # Initialize analyzer
    analyzer = ParticipantAnalyzer()

    # Run the complete analysis
    success = analyzer.run_analysis()

    if success:
        print("\nğŸ‰ All participant reports have been generated successfully!")
    else:
        print("\nâŒ Analysis failed. Please check the error messages above.")

    return success


if __name__ == "__main__":
    main()
